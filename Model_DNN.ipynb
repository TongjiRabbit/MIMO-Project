{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNecdK8qbZGz",
        "outputId": "caf0deea-af55-4f7d-d3eb-4affb80b8a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/MIMO_Project'"
      ],
      "metadata": {
        "id": "1j7oIx2g55Eb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.chdir(WORK_DIR)\n",
        "    print(f\"✅ 当前工作目录已切换至: {os.getcwd()}\")\n",
        "except OSError:\n",
        "    print(f\"❌ 路径不存在: {WORK_DIR}，请检查路径拼写！\")\n",
        "# Note: User's query refers to an IndentationError in cell S7ykF6D18U8K, not in this cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6z_UMms6Haz",
        "outputId": "2fe4fe7c-80cd-4b4c-9c26-9c69d6b4c83f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 当前工作目录已切换至: /content/drive/MyDrive/MIMO_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 步骤 1: 导入依赖与配置\n",
        "# ==========================================\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import glob"
      ],
      "metadata": {
        "id": "3ext0-mR6KRE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练超参数\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "EPOCHS = 1000\n",
        "HIDDEN_SIZE = 4096"
      ],
      "metadata": {
        "id": "HViJm8Q56Tab"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 选择设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"当前运行设备: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doLXqQBG6lzx",
        "outputId": "604e8dce-f9da-4321-fd9d-87d54ed54828"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前运行设备: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 步骤 2: 定义模型\n",
        "# ==========================================\n",
        "class ChannelNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ChannelNet, self).__init__()\n",
        "        # 第一层增加宽度，使用平滑降维\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 8192),\n",
        "            nn.BatchNorm1d(8192),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(8192, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        # 输出层\n",
        "        self.output_layer = nn.Linear(4096, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# ==========================================\n",
        "# 步骤 3: 数据加载 (内存优化版)\n",
        "# ==========================================\n",
        "def load_new_format_data():\n",
        "    \"\"\"\n",
        "    适配 Colab:\n",
        "    1. 使用 os.path.join 拼接路径\n",
        "    2. 返回 CPU Tensor，不要立刻传到 GPU\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "\n",
        "    # 在当前工作目录(WORK_DIR)下查找\n",
        "    file_pattern = \"TrainData_Batch_*.mat\"\n",
        "    files = glob.glob(file_pattern)\n",
        "\n",
        "    if not files:\n",
        "        raise ValueError(f\"❌ 在 {os.getcwd()} 下未找到任何匹配文件: {file_pattern}\")\n",
        "\n",
        "    print(f\"\\n>>> 发现 {len(files)} 个数据文件，开始加载...\")\n",
        "\n",
        "    count = 0\n",
        "    for filename in files:\n",
        "        try:\n",
        "            mat_data = sio.loadmat(filename)\n",
        "            if 'Batch_Buffer' not in mat_data:\n",
        "                continue\n",
        "\n",
        "            batch_buffer = mat_data['Batch_Buffer']\n",
        "            num_samples = batch_buffer.shape[1]\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                sample = batch_buffer[0, i]\n",
        "\n",
        "                # 提取数据\n",
        "                r_real = sample['R_Real']\n",
        "                r_imag = sample['R_Imag']\n",
        "                p_real = sample['P_Real']\n",
        "                p_imag = sample['P_Imag']\n",
        "\n",
        "                # Flatten\n",
        "                x_vec = np.concatenate((r_real.flatten(), r_imag.flatten()))\n",
        "                y_vec = np.concatenate((p_real.flatten(), p_imag.flatten()))\n",
        "\n",
        "                X_list.append(x_vec)\n",
        "                Y_list.append(y_vec)\n",
        "                count += 1\n",
        "\n",
        "            print(f\"   [已加载] {os.path.basename(filename)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   [错误] 读取 {filename} 失败: {e}\")\n",
        "\n",
        "    if count == 0:\n",
        "        raise ValueError(\"❌ 未成功加载任何样本数据！\")\n",
        "\n",
        "    X_all = np.array(X_list)\n",
        "    Y_all = np.array(Y_list)\n",
        "\n",
        "    print(f\"\\n✅ 数据加载完毕！\")\n",
        "    print(f\"   总样本数: {X_all.shape[0]}\")\n",
        "    print(f\"   输入维度: {X_all.shape[1]}\")\n",
        "    print(f\"   输出维度: {Y_all.shape[1]}\")\n",
        "\n",
        "    # 【关键修改】保留在 CPU 上，不要在这里 .to(device)\n",
        "    # 你的数据量很大，全部放入 GPU 可能会 OOM (Out of Memory)\n",
        "    X_tensor = torch.FloatTensor(X_all)\n",
        "    Y_tensor = torch.FloatTensor(Y_all)\n",
        "\n",
        "    return X_tensor, Y_tensor\n",
        "\n",
        "# ==========================================\n",
        "# 步骤 4: 训练循环\n",
        "# ==========================================\n",
        "def train():\n",
        "    # 1. 加载数据 (CPU)\n",
        "    X_train, Y_train = load_new_format_data()\n",
        "\n",
        "    # 2. 创建 DataLoader\n",
        "    dataset = TensorDataset(X_train, Y_train)\n",
        "    # pin_memory=True 可以加速数据从 CPU 到 GPU 的传输\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = Y_train.shape[1]\n",
        "\n",
        "    # 3. 初始化模型 (GPU)\n",
        "    model = ChannelNet(input_dim, output_dim).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    print(f\"\\n=== 开始训练 DNN 模型 ===\")\n",
        "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} Million\")\n",
        "    model.train()\n",
        "\n",
        "    # 混合 Loss 定义\n",
        "    class CombinedLoss(nn.Module):\n",
        "        def __init__(self, alpha=0.1):\n",
        "            super(CombinedLoss, self).__init__()\n",
        "            self.mse = nn.MSELoss()\n",
        "            self.cosine = nn.CosineSimilarity(dim=1)\n",
        "            self.alpha = alpha\n",
        "\n",
        "        def forward(self, pred, target):\n",
        "            loss_mse = self.mse(pred, target)\n",
        "            sim = self.cosine(pred, target)\n",
        "            loss_cos = 1.0 - torch.mean(sim)\n",
        "            return loss_mse + self.alpha * loss_cos\n",
        "\n",
        "    # 使用混合 Loss (如果你只想用 MSE，可以用上面的 criterion)\n",
        "    loss_fn = CombinedLoss(alpha=0.1)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch_x, batch_y in loader:\n",
        "            # 【关键修改】在每个 Batch 训练时再移动到 GPU\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "\n",
        "            # 计算 Loss\n",
        "            loss = loss_fn(output, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            avg_loss = total_loss / len(loader)\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    print(\"训练完成！\")\n",
        "\n",
        "    # 保存模型 (直接保存到云盘 WORK_DIR)\n",
        "    save_name = f'DFDCA_DNN_Model_TrainBatch1to5_4096hidden_500epoch.pth'\n",
        "    torch.save(model.state_dict(), os.path.join(WORK_DIR, save_name))\n",
        "    print(f\"✅ 模型已保存至云盘: {os.path.join(WORK_DIR, save_name)}\")"
      ],
      "metadata": {
        "id": "OF0XgcBf8UP3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "S7ykF6D18U8K",
        "outputId": "fbd652b6-0717-4897-9a89-6441144cb63a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> 发现 5 个数据文件，开始加载...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1648720672.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2842932298.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# 1. 加载数据 (CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_new_format_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# 2. 创建 DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2842932298.py\u001b[0m in \u001b[0;36mload_new_format_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mmat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'Batch_Buffer'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmat_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    289\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         '''\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}